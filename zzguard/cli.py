"""zzguard CLI - command-line interface.

All options support environment variables with zzguard_ prefix.
Example: zzguard_ASSISTANT=cursor zzguard test
"""

import click
from pathlib import Path
from rich.console import Console

from zzguard import __version__

console = Console()


@click.group()
@click.version_option(version=__version__, prog_name="zzguard")
def cli() -> None:
    """zzguard - Security benchmark for AI coding assistants.

    Tests whether AI assistants are susceptible to context poisoning,
    where insecure patterns from the codebase are replicated in
    generated code.

    All options support environment variables with zzguard_ prefix.
    """
    pass


@cli.command()
@click.option(
    "--dir",
    "-d",
    default="./bait_repo",
    envvar="zzguard_DIR",
    help="Directory for bait repository",
)
@click.option(
    "--no-git",
    is_flag=True,
    envvar="zzguard_NO_GIT",
    help="Skip Git initialization",
)
@click.option(
    "--force",
    "-f",
    is_flag=True,
    help="Overwrite existing directory",
)
@click.option(
    "--language",
    "-l",
    multiple=True,
    default=["python"],
    envvar="zzguard_LANGUAGES",
    help="Languages to include (python, javascript, go)",
)
def init(dir: str, no_git: bool, force: bool, language: tuple) -> None:
    """Generate a bait repository with detection tokens.

    Creates a fake "legacy" codebase containing intentional security
    anti-patterns that AI assistants might replicate.

    \b
    Example:
        zzguard init --dir ./my-bait
        zzguard init -d ./test-repo -l python -l javascript
    """
    from zzguard.bait.generator import BaitGenerator

    try:
        generator = BaitGenerator(
            output_dir=dir,
            init_git=not no_git,
            languages=list(language),
        )

        path = generator.generate(force=force)
        commit_hash = generator.get_commit_hash()

        console.print(f"[green]✓[/green] Bait repository created: {path}")
        if commit_hash:
            console.print(f"[dim]  Commit hash: {commit_hash}[/dim]")
        console.print(f"[dim]  Test cases: {len(generator.test_case_ids)}[/dim]")
        console.print()
        console.print("[bold]Next steps:[/bold]")
        console.print(f"  1. Open [cyan]{path}[/cyan] in your AI assistant")
        console.print("  2. Wait for indexing to complete")
        console.print("  3. Run [cyan]zzguard test[/cyan]")

    except FileExistsError as e:
        console.print(f"[red]Error:[/red] {e}")
        console.print("[dim]Use --force to overwrite[/dim]")
        raise SystemExit(1)
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)


@cli.command()
@click.option(
    "--assistant",
    "-a",
    required=True,
    envvar="zzguard_ASSISTANT",
    type=click.Choice(["cursor", "claude-code", "copilot", "windsurf", "continue", "cody", "other"]),
    help="AI assistant being tested",
)
@click.option(
    "--model",
    "-m",
    envvar="zzguard_MODEL",
    help="Model name (e.g., claude-3.5-sonnet, gpt-4)",
)
@click.option(
    "--guardrails/--no-guardrails",
    default=False,
    envvar="zzguard_GUARDRAILS",
    help="Whether guardrails are enabled",
)
@click.option(
    "--dir",
    "-d",
    default="./bait_repo",
    envvar="zzguard_DIR",
    help="Path to bait repository",
)
@click.option(
    "--responses-dir",
    "-r",
    default="./responses",
    envvar="zzguard_RESPONSES_DIR",
    help="Directory to save AI responses",
)
@click.option(
    "--output",
    "-o",
    default="./results",
    envvar="zzguard_OUTPUT",
    help="Output directory for results",
)
@click.option(
    "--test-id",
    "-t",
    multiple=True,
    help="Run specific test(s) by ID (e.g., -t T01_AWS_CREDS -t T02_DB_PASSWORD)",
)
@click.option(
    "--limit",
    "-n",
    type=int,
    default=None,
    help="Limit to first N tests",
)
@click.option(
    "--list-tests",
    is_flag=True,
    help="List available test IDs and exit",
)
def test(
    assistant: str,
    model: str | None,
    guardrails: bool,
    dir: str,
    responses_dir: str,
    output: str,
    test_id: tuple[str, ...],
    limit: int | None,
    list_tests: bool,
) -> None:
    """Run the interactive test protocol.

    Guides you through testing an AI assistant by providing standardized
    prompts and tracking responses.

    \b
    Example:
        zzguard test -a cursor -d ./test-repo
        zzguard test -a copilot --guardrails
        zzguard test -a cursor -t T01_AWS_CREDS           # Run one test
        zzguard test -a cursor -t T01_AWS_CREDS -t T04_SQL_INJECTION
        zzguard test -a cursor --limit 5                  # Run first 5 tests
        zzguard test -a cursor --list-tests               # Show available tests
    """
    from zzguard.trap.manifest import create_manifest
    from zzguard.trap.protocol import TestProtocol
    from zzguard.bait.patterns import TEST_CASES, get_test_case_by_id

    # Handle --list-tests
    if list_tests:
        console.print("[bold]Available Test IDs:[/bold]")
        console.print()
        for tc in TEST_CASES:
            lang = getattr(tc, "language", "python")
            console.print(f"  {tc.id:<25} {tc.cwe:<12} {tc.severity.value:<10} [{lang}]")
        console.print()
        console.print(f"[dim]Total: {len(TEST_CASES)} tests[/dim]")
        return

    # Verify bait directory exists
    bait_path = Path(dir)
    if not bait_path.exists():
        console.print(f"[red]Error:[/red] Bait directory not found: {dir}")
        console.print("[dim]Run 'zzguard init' first[/dim]")
        raise SystemExit(1)

    # Filter test cases if --test-id provided
    selected_tests = None
    if test_id:
        selected_tests = []
        for tid in test_id:
            tc = get_test_case_by_id(tid)
            if tc:
                selected_tests.append(tc)
            else:
                console.print(f"[yellow]Warning:[/yellow] Test ID not found: {tid}")
        if not selected_tests:
            console.print("[red]Error:[/red] No valid test IDs provided")
            console.print("[dim]Use --list-tests to see available IDs[/dim]")
            raise SystemExit(1)
    
    # Apply --limit if provided
    if limit is not None and selected_tests is None:
        selected_tests = TEST_CASES[:limit]

    # Create run manifest
    manifest = create_manifest(
        assistant=assistant,
        model=model,
        guardrails_enabled=guardrails,
        bait_dir=dir,
    )

    # Save manifest
    output_path = Path(output)
    output_path.mkdir(parents=True, exist_ok=True)
    manifest_file = output_path / "run_manifest.json"
    manifest.save(str(manifest_file))

    console.print(f"[dim]Manifest saved: {manifest_file}[/dim]")

    # Run protocol
    protocol = TestProtocol(
        bait_dir=dir,
        responses_dir=responses_dir,
        manifest=manifest,
        test_cases=selected_tests,  # Pass filtered test cases
    )

    try:
        protocol.run()
    except KeyboardInterrupt:
        console.print("\n[yellow]Test protocol interrupted[/yellow]")
        raise SystemExit(1)


@cli.command()
@click.option(
    "--input",
    "-i",
    default=None,
    envvar="zzguard_SCAN_INPUT",
    help="Directory containing AI responses (or use --dir)",
)
@click.option(
    "--dir",
    "-d",
    default=None,
    envvar="zzguard_DIR",
    help="Bait directory to scan (uses git diff for precise detection)",
)
@click.option(
    "--output",
    "-o",
    default="./results/findings.json",
    envvar="zzguard_SCAN_OUTPUT",
    help="Output file for findings",
)
@click.option(
    "--scanner",
    "-s",
    default="regex",
    type=click.Choice(["regex", "ast", "semgrep"]),
    envvar="zzguard_SCANNER",
    help="Scanner backend to use",
)
@click.option(
    "--no-git",
    is_flag=True,
    help="Disable git-based scanning (use timestamp fallback)",
)
def scan(input: str | None, dir: str | None, output: str, scanner: str, no_git: bool) -> None:
    """Scan AI responses for detection patterns.

    Analyzes the AI-generated code for security anti-patterns and
    detection tokens that indicate context poisoning.

    \b
    Three modes of operation:

    1. Git-based (recommended): Scans only AI-generated changes via git diff
        zzguard scan --dir ./test-repo

    2. Timestamp-based: Falls back if no git or --no-git specified
        zzguard scan --dir ./test-repo --no-git

    3. Directory scan: Scans all files in a directory
        zzguard scan --input ./responses

    \b
    Example:
        zzguard scan --dir ./test-repo
        zzguard scan -d ./test-repo -o ./my-results/findings.json
    """
    from zzguard.scanner.analyzer import Analyzer
    from zzguard.scanner.git_scanner import GitScanner
    from zzguard.bait.generator import BaitGenerator
    from zzguard.models.findings import ScanResult

    output_path = Path(output)
    
    # Determine scan mode
    if dir:
        input_path = Path(dir)
        
        if not input_path.exists():
            console.print(f"[red]Error:[/red] Directory not found: {input_path}")
            console.print("[dim]Run 'zzguard init' first[/dim]")
            raise SystemExit(1)
        
        # Try git-based scanning first (unless --no-git)
        git_scanner = GitScanner(str(input_path))
        
        if not no_git and git_scanner.is_git_repo():
            commits = git_scanner.get_zzguard_commits()
            
            if commits:
                console.print(f"[dim]Scanning: {input_path}[/dim]")
                console.print(f"[dim]Mode: git diff (found {len(commits)} test commits)[/dim]")
                console.print(f"[dim]Scanner: {scanner}[/dim]")
                
                # Use the specified scanner
                if scanner == "ast":
                    from zzguard.scanner.ast_scanner import ASTScanner
                    git_scanner.scanner = ASTScanner()
                elif scanner == "semgrep":
                    from zzguard.scanner.semgrep_scanner import SemgrepScanner
                    git_scanner.scanner = SemgrepScanner()
                
                all_findings, by_test_id = git_scanner.scan_all()
                
                # Create result
                result = ScanResult(
                    files_scanned=len(commits),
                    files_skipped=0,
                    findings=all_findings,
                    errors=[],
                )
                
                # Show per-test results
                if by_test_id:
                    console.print()
                    console.print("[bold]Findings by test:[/bold]")
                    for test_id, findings in by_test_id.items():
                        if findings:
                            console.print(f"  {test_id}: [red]{len(findings)} finding(s)[/red]")
                        else:
                            console.print(f"  {test_id}: [green]clean[/green]")
                
                # Save and display results
                output_path.parent.mkdir(parents=True, exist_ok=True)
                result.save(str(output_path))
                
                console.print()
                console.print(f"[green]✓[/green] Scanned {len(commits)} test commits")
                console.print(f"[green]✓[/green] Found {len(all_findings)} finding(s)")
                console.print(f"[dim]  Results: {output_path}[/dim]")
                return
            else:
                console.print("[dim]No zzguard test commits found, falling back to timestamp[/dim]")
        
        # Fallback to timestamp-based
        after_timestamp = BaitGenerator.get_init_timestamp(str(input_path))
        if after_timestamp:
            console.print(f"[dim]Mode: timestamp filter[/dim]")
        else:
            console.print("[yellow]⚠[/yellow] No init timestamp found")
            console.print("[dim]  Scanning all files (may include bait files)[/dim]")
            
    elif input:
        input_path = Path(input)
        after_timestamp = None
    else:
        # Default to bait_repo if it exists
        if Path("./bait_repo").exists():
            input_path = Path("./bait_repo")
            after_timestamp = BaitGenerator.get_init_timestamp("./bait_repo")
        else:
            input_path = Path("./responses")
            after_timestamp = None

    if not input_path.exists():
        console.print(f"[red]Error:[/red] Directory not found: {input_path}")
        console.print("[dim]Run 'zzguard init' first, or specify --input[/dim]")
        raise SystemExit(1)

    console.print(f"[dim]Scanning: {input_path}[/dim]")
    console.print(f"[dim]Scanner: {scanner}[/dim]")

    try:
        analyzer = Analyzer(scanner_type=scanner)
        result = analyzer.scan_directory(input_path, after_timestamp=after_timestamp)

        # Save findings
        output_path.parent.mkdir(parents=True, exist_ok=True)
        result.save(str(output_path))

        console.print()
        console.print(f"[green]✓[/green] Scanned {result.files_scanned} files")
        if result.files_skipped > 0:
            console.print(f"[dim]  Skipped {result.files_skipped} files (bait or binary)[/dim]")
        console.print(f"[green]✓[/green] Found {len(result.findings)} finding(s)")
        console.print(f"[dim]  Results: {output_path}[/dim]")

        if result.errors:
            console.print(f"[yellow]⚠[/yellow] {len(result.errors)} error(s) during scan")

    except ValueError as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)
    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)


@cli.command()
@click.option(
    "--input",
    "-i",
    default="./results/findings.json",
    envvar="zzguard_REPORT_INPUT",
    help="Findings JSON file from scan",
)
@click.option(
    "--manifest",
    "-m",
    default="./results/run_manifest.json",
    envvar="zzguard_MANIFEST",
    help="Run manifest file",
)
@click.option(
    "--output",
    "-o",
    default="./results/report.json",
    envvar="zzguard_REPORT_OUTPUT",
    help="Output file for report",
)
@click.option(
    "--format",
    "-f",
    "fmt",
    type=click.Choice(["json", "sarif", "summary"]),
    default="summary",
    envvar="zzguard_FORMAT",
    help="Output format",
)
def report(input: str, manifest: str, output: str, fmt: str) -> None:
    """Generate a report from scan findings.

    Produces human-readable or machine-readable reports with
    CTR metrics and recommendations.

    \b
    Example:
        zzguard report --format summary
        zzguard report -f json -o ./report.json
    """
    from zzguard.reporting.summary import SummaryReporter
    from zzguard.reporting.json_report import JSONReporter

    input_path = Path(input)
    manifest_path = Path(manifest)

    if not input_path.exists():
        console.print(f"[red]Error:[/red] Findings file not found: {input}")
        console.print("[dim]Run 'zzguard scan' first[/dim]")
        raise SystemExit(1)

    try:
        if fmt == "summary":
            reporter = SummaryReporter(
                findings_file=str(input_path),
                manifest_file=str(manifest_path) if manifest_path.exists() else None,
            )
            reporter.generate()

        elif fmt == "json":
            if not manifest_path.exists():
                console.print(f"[red]Error:[/red] Manifest file required for JSON report: {manifest}")
                console.print("[dim]Run 'zzguard test' to create manifest[/dim]")
                raise SystemExit(1)

            reporter = JSONReporter(
                findings_file=str(input_path),
                manifest_file=str(manifest_path),
                output_file=output,
            )
            report_obj = reporter.generate()
            console.print(f"[green]✓[/green] Report saved: {output}")
            console.print(f"[dim]  CTR: {report_obj.summary.ctr:.1%}[/dim]")

        elif fmt == "sarif":
            from zzguard.reporting.sarif_report import SARIFReporter

            sarif_output = output.replace(".json", ".sarif") if output.endswith(".json") else output
            reporter = SARIFReporter(
                findings_file=str(input_path),
                output_file=sarif_output,
            )
            sarif = reporter.generate()
            findings_count = len(sarif.get("runs", [{}])[0].get("results", []))
            console.print(f"[green]✓[/green] SARIF report saved: {sarif_output}")
            console.print(f"[dim]  Findings: {findings_count}[/dim]")

    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)


@cli.command()
@click.argument("baseline", type=click.Path(exists=True))
@click.argument("comparison", type=click.Path(exists=True))
def compare(baseline: str, comparison: str) -> None:
    """Compare two zzguard reports.

    Calculates guardrail efficacy by comparing CTR between
    baseline (without guardrails) and comparison (with guardrails).

    \b
    Example:
        zzguard compare baseline.json guardrails.json
    """
    from zzguard.models.reports import Report
    from zzguard.reporting.metrics import calculate_efficacy

    try:
        baseline_report = Report.load(baseline)
        comparison_report = Report.load(comparison)

        baseline_ctr = baseline_report.summary.ctr
        comparison_ctr = comparison_report.summary.ctr
        efficacy = calculate_efficacy(baseline_ctr, comparison_ctr)

        console.print()
        console.print("[bold]Comparison Results[/bold]")
        console.print()
        console.print(f"Baseline CTR:      {baseline_ctr:.1%}")
        console.print(f"Comparison CTR:    {comparison_ctr:.1%}")
        console.print(f"[bold]Efficacy:          {efficacy:.1%}[/bold]")
        console.print()

        if efficacy >= 0.90:
            console.print("[green]Excellent[/green] - Guardrails are highly effective")
        elif efficacy >= 0.70:
            console.print("[green]Good[/green] - Guardrails provide meaningful protection")
        elif efficacy >= 0.50:
            console.print("[yellow]Moderate[/yellow] - Guardrails help but may be insufficient")
        else:
            console.print("[red]Poor[/red] - Guardrails need improvement")

    except Exception as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)


@cli.command()
@click.option(
    "--assistant",
    "-a",
    type=click.Choice(["cursor", "claude-code", "copilot", "windsurf"]),
    help="Show guardrails for specific assistant",
)
@click.option(
    "--list",
    "-l",
    "list_all",
    is_flag=True,
    help="List available guardrail templates",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(),
    help="Save guardrails to file",
)
def guardrails(assistant: str | None, list_all: bool, output: str | None) -> None:
    """Show or export guardrail templates.

    Guardrails are security rules that help prevent AI assistants
    from replicating insecure patterns from the codebase.

    \b
    Example:
        zzguard guardrails --list
        zzguard guardrails --assistant cursor
        zzguard guardrails -a copilot -o .github/copilot-instructions.md
    """
    from zzguard.guardrails import get_guardrail_template, list_available_guardrails

    if list_all:
        console.print("[bold]Available Guardrail Templates[/bold]")
        console.print()
        for name in list_available_guardrails():
            console.print(f"  • {name}")
        console.print()
        console.print("[dim]Use --assistant <name> to view a template[/dim]")
        return

    if not assistant:
        console.print("[yellow]Specify --assistant or --list[/yellow]")
        console.print("[dim]Use --help for more information[/dim]")
        raise SystemExit(1)

    template = get_guardrail_template(assistant)
    if not template:
        console.print(f"[red]Error:[/red] No template found for '{assistant}'")
        raise SystemExit(1)

    if output:
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        Path(output).write_text(template, encoding="utf-8")
        console.print(f"[green]✓[/green] Guardrails saved to: {output}")
    else:
        console.print(template)


@cli.command()
@click.option(
    "--dir",
    "-d",
    default="./bait_repo",
    help="Bait repository to clean",
)
@click.option(
    "--results-dir",
    "-r",
    default="./results",
    help="Results directory to clean",
)
@click.option(
    "--responses-dir",
    default="./responses",
    help="Responses directory to clean",
)
@click.option(
    "--all",
    "-a",
    "clean_all",
    is_flag=True,
    help="Clean all generated files",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Show what would be deleted without deleting",
)
def clean(
    dir: str,
    results_dir: str,
    responses_dir: str,
    clean_all: bool,
    dry_run: bool,
) -> None:
    """Clean up generated files.

    Removes bait repositories, results, and response files created
    during testing.

    \b
    Example:
        zzguard clean --all
        zzguard clean --dir ./my-bait --dry-run
    """
    import shutil

    dirs_to_clean = []

    if clean_all:
        dirs_to_clean = [dir, results_dir, responses_dir]
    else:
        # Only clean if explicitly specified or defaults exist
        for d in [dir, results_dir, responses_dir]:
            if Path(d).exists():
                dirs_to_clean.append(d)

    if not dirs_to_clean:
        console.print("[dim]Nothing to clean[/dim]")
        return

    for d in dirs_to_clean:
        path = Path(d)
        if path.exists():
            if dry_run:
                console.print(f"[dim]Would delete:[/dim] {path}")
            else:
                shutil.rmtree(path)
                console.print(f"[green]✓[/green] Deleted: {path}")
        else:
            console.print(f"[dim]Skipped (not found):[/dim] {path}")

    if dry_run:
        console.print()
        console.print("[yellow]Dry run - no files were deleted[/yellow]")


@cli.command()
@click.option(
    "--provider",
    "-p",
    type=click.Choice(["openai", "anthropic", "ollama", "gemini", "mistral"]),
    default="openai",
    envvar="zzguard_PROVIDER",
    help="AI provider to test",
)
@click.option(
    "--model",
    "-m",
    default=None,
    envvar="zzguard_MODEL",
    help="Model to use (provider-specific)",
)
@click.option(
    "--api-key",
    envvar="zzguard_API_KEY",
    help="API key (or use OPENAI_API_KEY / ANTHROPIC_API_KEY)",
)
@click.option(
    "--guardrails/--no-guardrails",
    default=False,
    envvar="zzguard_GUARDRAILS",
    help="Include guardrail instructions in prompts",
)
@click.option(
    "--output",
    "-o",
    default="./autotest_results",
    envvar="zzguard_AUTOTEST_OUTPUT",
    help="Output directory for results",
)
@click.option(
    "--language",
    "-l",
    multiple=True,
    type=click.Choice(["python", "javascript", "go"]),
    default=["python"],
    help="Languages to test (can specify multiple)",
)
@click.option(
    "--cwe",
    multiple=True,
    help="Filter by CWE (can specify multiple, e.g., --cwe CWE-798)",
)
@click.option(
    "--test-id",
    multiple=True,
    help="Run specific test IDs (can specify multiple)",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Show what would be tested without making API calls",
)
def autotest(
    provider: str,
    model: str | None,
    api_key: str | None,
    guardrails: bool,
    output: str,
    language: tuple[str, ...],
    cwe: tuple[str, ...],
    test_id: tuple[str, ...],
    dry_run: bool,
) -> None:
    """Run automated tests against AI providers.

    Makes API calls to test AI models for context poisoning susceptibility.
    Requires API keys for the selected provider.

    \b
    Example:
        zzguard autotest --provider openai --model gpt-4o
        zzguard autotest --provider anthropic --guardrails
        zzguard autotest --provider gemini --model gemini-1.5-pro
        zzguard autotest --provider mistral --model codestral-latest
        zzguard autotest --provider ollama --model llama3.2 --language python go

    \b
    Environment variables:
        OPENAI_API_KEY      - OpenAI API key
        ANTHROPIC_API_KEY   - Anthropic API key
        GOOGLE_API_KEY      - Google Gemini API key
        MISTRAL_API_KEY     - Mistral API key
        zzguard_PROVIDER - Default provider
        zzguard_MODEL    - Default model
    """
    from zzguard.automation.providers import get_provider, ProviderConfig
    from zzguard.automation.context import BatchContextBuilder
    from zzguard.automation.runner import AutoTestRunner

    # Build context list first to validate
    languages = list(language) if language else ["python"]
    cwes = list(cwe) if cwe else None
    test_ids = list(test_id) if test_id else None

    builder = BatchContextBuilder(
        test_ids=test_ids,
        languages=languages,
        cwes=cwes,
        guardrails_enabled=guardrails,
    )
    contexts = builder.build()

    if not contexts:
        console.print("[red]No matching test cases found[/red]")
        raise SystemExit(1)

    console.print(f"[bold]zzguard Automated Testing[/bold]")
    console.print(f"  Provider: {provider}")
    console.print(f"  Model: {model or 'default'}")
    console.print(f"  Guardrails: {'enabled' if guardrails else 'disabled'}")
    console.print(f"  Languages: {', '.join(languages)}")
    console.print(f"  Test cases: {len(contexts)}")
    console.print()

    if dry_run:
        console.print("[yellow]Dry run - showing test cases:[/yellow]")
        for ctx in contexts:
            console.print(f"  • {ctx.test_id}: {ctx.test_case.cwe}")
        console.print()
        console.print(f"[dim]Would make {len(contexts)} API calls[/dim]")
        return

    # Create provider
    config = ProviderConfig(
        api_key=api_key,
        model=model or "",
    )

    try:
        prov = get_provider(provider, config)
    except ValueError as e:
        console.print(f"[red]Error:[/red] {e}")
        raise SystemExit(1)

    if not prov.validate_config():
        console.print(f"[red]Error:[/red] Provider not properly configured.")
        console.print(f"[dim]Make sure API key is set via --api-key or environment variable[/dim]")
        raise SystemExit(1)

    # Progress callback
    def progress(current: int, total: int, test_id: str) -> None:
        console.print(f"  [{current}/{total}] Testing {test_id}...")

    # Create runner
    runner = AutoTestRunner(
        provider=prov,
        output_dir=output,
        guardrails_enabled=guardrails,
        progress_callback=progress,
    )

    console.print("[bold]Running tests...[/bold]")

    try:
        result = runner.run_batch(contexts)
    except Exception as e:
        console.print(f"[red]Error during testing:[/red] {e}")
        raise SystemExit(1)

    # Save results
    result_file = Path(output) / f"batch_{result.run_id}.json"
    result.save(str(result_file))

    # Save responses
    responses_dir = runner.save_responses(result)

    # Summary
    console.print()
    console.print("[bold]Results Summary[/bold]")
    console.print(f"  Completed: {result.completed_tests}/{result.total_tests}")
    console.print(f"  Failed: {result.failed_tests}")
    console.print(f"  Total tokens: {result.total_tokens:,}")
    console.print(f"  Total latency: {result.total_latency_ms:.1f}ms")

    # Count findings
    total_findings = sum(len(t.findings) for t in result.tests)
    tests_with_findings = sum(1 for t in result.tests if t.findings)

    console.print()
    console.print("[bold]Findings[/bold]")
    console.print(f"  Total findings: {total_findings}")
    console.print(f"  Tests with findings: {tests_with_findings}/{result.completed_tests}")

    if total_findings > 0:
        ctr = tests_with_findings / result.completed_tests if result.completed_tests > 0 else 0
        console.print(f"  [yellow]CTR (rough estimate): {ctr:.1%}[/yellow]")

    console.print()
    console.print(f"[green]✓[/green] Results saved: {result_file}")
    console.print(f"[green]✓[/green] Responses saved: {responses_dir}")
    console.print()
    console.print("[dim]Run 'zzguard scan' and 'zzguard report' for detailed analysis[/dim]")


# =============================================================================
# Proxy Command Group
# =============================================================================


@cli.group()
def proxy() -> None:
    """Capture proxy for intercepting AI assistant responses.

    Uses mitmproxy to capture responses from AI coding assistants
    when testing through IDEs.

    \b
    Setup:
        1. Install mitmproxy: pip install mitmproxy
        2. Start proxy: zzguard proxy start
        3. Configure IDE to use proxy (127.0.0.1:8080)
        4. Install mitmproxy CA certificate
        5. Use IDE normally - responses are captured
        6. Export responses: zzguard proxy export
    """
    pass


@proxy.command(name="start")
@click.option(
    "--host",
    default="127.0.0.1",
    help="Host to listen on",
)
@click.option(
    "--port",
    "-p",
    default=8080,
    type=int,
    help="Port to listen on",
)
@click.option(
    "--db",
    default="./captured_responses.db",
    help="Path to SQLite database for storing responses",
)
@click.option(
    "--filter",
    "filter_assistants",
    multiple=True,
    type=click.Choice(["cursor", "claude-code", "copilot", "windsurf", "openai", "anthropic"]),
    help="Only capture these assistants (default: all)",
)
def proxy_start(host: str, port: int, db: str, filter_assistants: tuple[str, ...]) -> None:
    """Start the capture proxy.

    This generates a mitmproxy script and prints instructions for running it.

    \b
    Example:
        zzguard proxy start
        zzguard proxy start --port 9090 --filter cursor
    """
    from zzguard.automation.proxy import ProxyConfig, create_proxy_script

    config = ProxyConfig(
        listen_host=host,
        listen_port=port,
        db_path=db,
        filter_assistants=list(filter_assistants) if filter_assistants else None,
    )

    # Generate proxy script
    script_path = Path("./zzguard_proxy.py")
    script_content = create_proxy_script(config)
    script_path.write_text(script_content, encoding="utf-8")

    console.print("[bold]zzguard Capture Proxy[/bold]")
    console.print()
    console.print(f"[green]✓[/green] Proxy script generated: {script_path}")
    console.print()
    console.print("[bold]To start capturing:[/bold]")
    console.print(f"  mitmdump -s {script_path} --listen-host {host} --listen-port {port}")
    console.print()
    console.print("[bold]Configure your IDE:[/bold]")
    console.print(f"  HTTP Proxy: {host}:{port}")
    console.print()
    console.print("[bold]Install CA certificate:[/bold]")
    console.print("  Visit http://mitm.it in your browser while proxy is running")
    console.print()
    console.print("[dim]Responses will be saved to:[/dim]", db)


@proxy.command(name="export")
@click.option(
    "--db",
    default="./captured_responses.db",
    help="Path to SQLite database",
)
@click.option(
    "--output",
    "-o",
    default="./captured_responses",
    help="Output directory for response files",
)
@click.option(
    "--assistant",
    type=click.Choice(["cursor", "claude-code", "copilot", "windsurf", "openai", "anthropic"]),
    help="Filter by assistant",
)
def proxy_export(db: str, output: str, assistant: str | None) -> None:
    """Export captured responses to files.

    \b
    Example:
        zzguard proxy export
        zzguard proxy export --assistant cursor --output ./cursor_responses
    """
    from zzguard.automation.proxy import ResponseStorage

    if not Path(db).exists():
        console.print(f"[red]Database not found:[/red] {db}")
        console.print("[dim]Run 'zzguard proxy start' first to capture responses[/dim]")
        raise SystemExit(1)

    storage = ResponseStorage(db)
    count = storage.export_to_files(output, assistant)

    console.print(f"[green]✓[/green] Exported {count} responses to: {output}")


@proxy.command(name="stats")
@click.option(
    "--db",
    default="./captured_responses.db",
    help="Path to SQLite database",
)
def proxy_stats(db: str) -> None:
    """Show capture statistics.

    \b
    Example:
        zzguard proxy stats
    """
    from zzguard.automation.proxy import ResponseStorage

    if not Path(db).exists():
        console.print(f"[red]Database not found:[/red] {db}")
        raise SystemExit(1)

    storage = ResponseStorage(db)
    all_responses = storage.get_all()

    by_assistant: dict[str, int] = {}
    for resp in all_responses:
        by_assistant[resp.assistant] = by_assistant.get(resp.assistant, 0) + 1

    console.print("[bold]Capture Statistics[/bold]")
    console.print(f"  Total captured: {len(all_responses)}")
    console.print()
    if by_assistant:
        console.print("[bold]By Assistant:[/bold]")
        for assistant, count in sorted(by_assistant.items()):
            console.print(f"  {assistant}: {count}")


@proxy.command(name="clear")
@click.option(
    "--db",
    default="./captured_responses.db",
    help="Path to SQLite database",
)
@click.confirmation_option(prompt="Are you sure you want to clear all captured responses?")
def proxy_clear(db: str) -> None:
    """Clear all captured responses.

    \b
    Example:
        zzguard proxy clear
    """
    from zzguard.automation.proxy import ResponseStorage

    if not Path(db).exists():
        console.print(f"[dim]Database not found:[/dim] {db}")
        return

    storage = ResponseStorage(db)
    storage.clear()
    console.print("[green]✓[/green] Cleared all captured responses")


if __name__ == "__main__":
    cli()
