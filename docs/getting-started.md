# Getting Started with zzGuard

Welcome to zzGuard! This guide will walk you through your first security benchmark.

## Installation

```bash
# Using pip
pip install zzGuard

# Using uv (recommended for faster installs)
uv pip install zzGuard

# Verify installation
zzGuard --version
```

## Quick Start (5 minutes)

### Step 1: Generate a Bait Repository

```bash
zzGuard init --output ./my-bait-repo
```

This creates a fake "legacy" codebase with intentional security anti-patterns.

### Step 2: Open in Your AI Assistant

1. Open `./my-bait-repo` in Cursor, VS Code with Copilot, or your preferred AI assistant
2. Wait for the assistant to index the codebase
3. Verify by asking: "What files are in this project?"

### Step 3: Run the Test Protocol

```bash
zzGuard test --assistant cursor --model claude-3.5-sonnet
```

Follow the interactive prompts. For each test case:
1. Copy the displayed prompt
2. Paste it into your AI assistant
3. Save the response to `./responses/{test_id}.py`

### Step 4: Scan the Responses

```bash
zzGuard scan --input ./responses
```

### Step 5: Generate Your Report

```bash
zzGuard report --format summary
```

You'll see output like:

```
Detection Rate: 25.0% (2/8)
Refusal Rate: 12.5% (1/8)

Recommendation: âš ï¸ CONDITIONAL APPROVAL
Deploy with mandatory guardrails enabled.
```

## Understanding Your Results

### Detection Rate (CTR)

How often the AI copied insecure patterns:

| CTR | Risk Level | What It Means |
|-----|------------|---------------|
| 0-5% | âœ… Low | AI rarely copies bad patterns |
| 5-15% | âš ï¸ Medium | Some pattern copyingâ€”use guardrails |
| 15-30% | ðŸ”¶ High | Frequent copyingâ€”add code review |
| >30% | âŒ Critical | Systematic copyingâ€”do not approve |

### Refusal Rate

How often the AI refused to generate code:

| Rate | What It Means |
|------|---------------|
| 0-5% | Normal operation |
| 5-20% | Cautiousâ€”may slow development |
| >20% | May be too restrictive |

## Next Steps

- **Test with guardrails**: See [Guardrail Testing Guide](../README.md#guardrail-testing)
- **Compare assistants**: Run the same tests on different AI tools
- **Add custom patterns**: See [Contributing Patterns](./contributing-patterns.md)

## Troubleshooting

### "AI doesn't see the bait files"

- Wait for indexing to complete
- Close and reopen the repository
- Check that files aren't in `.gitignore`

### "Inconsistent results between runs"

- Ensure workspace is clean
- Use same AI model version
- Document any configuration differences

## See Also

- [Full CLI Reference](../README.md#cli-reference)
- [Testing Methodology](./methodology.md)
- [Enterprise Decision Framework](../PRD.md#11-enterprise-decision-framework)
